\documentclass[12pt]{article}
\usepackage{cite}

\title{Parse-tree-grams in word embedding.}
\author{Ilya Feldshteyn}

 
\begin{document}
\maketitle

\begin{abstract}
We are introducing parse-tree-grams (PT-grams) 
\end{abstract}


\section{Introduction}

Statistical methods of text analysis relay among other
text characteristics on word co-occurrences.
Words systematically found close to each other in texts
allows us to think about a common context. Word
embedding techniques -- word2vec and its modifications --
associate a word with a vector in a multi-dimensional
space and words used together in different texts are
expected to be close in that space.

This approach is easy to implement and scale,
but the view of a text as a flat flow of words has
certain drawbacks. Text has a hierarchical structure
and not all words co-occurrences are equally important.

To address this issue we consider a modification of
skip-gram that takes into account the hierarchical structure of the
text. Words co-occurrence is considered with respect to a
word position in a parse tree -- words form a parse-tree-gram 
(PT-gram) if they belong to the same sub-tree of a parse tree.
Instead of words in a fixed-size window we are first building
a parse tree of a sentence and analyze its structure.


To evaluate the effect of using PT-grams we've built
two word2vec embeddings -- using skip-grams and PT-grams representations --
and compared their performance on analogy test ([]).

\section{Calculations}

For the purposes of this study we were using relatively small
corpus of 1 million sentences from Wikipedia (???link),
Stanford CoreNLP (\cite{corenlp}) to build parse trees, and
Tensorflow (\cite{tensorflow}) for word2vec program.

Parse trees split into subtrees form variable-size PT-grams.
Average size of subtrees is 4.2 words.
This size was used as a reference to construct
window of comparable size (-2,+2) for skip-grams. 
PT-grams for the example above:


Two embeddings (300-dimensional space)
were generated with the same parameters and models were trained
for the same epochs.

\section{Results}

\section{Conclusion}




\bibliographystyle{unsrt}
\bibliography{ptgrams1}


\end{document}
