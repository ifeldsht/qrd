\documentclass[12pt]{article}
\usepackage{cite}
\usepackage{tikz}
\usepackage{tikz-qtree}

\title{Parse-tree-grams in word embedding.}
\author{Ilya Feldshteyn}

 
\begin{document}
\maketitle

\begin{abstract}
We are introducing parse-tree-grams (PT-grams) 
\end{abstract}


\section{Introduction}

Statistical methods of text analysis relay among other
text characteristics on word co-occurrences.
Words systematically found close to each other in texts
allows us to think about a common context. Word
embedding techniques -- word2vec and its modifications --
associate a word with a vector in a multi-dimensional
space and words used together in different texts are
expected to be close in that space.

This approach is easy to implement and scale,
but the view of a text as a flat flow of words has
certain drawbacks. Text has a hierarchical structure
and not all words co-occurrences are equally important.
The problem is not new, some efforts were made to take
into account the issue 
(see for example \cite{DBLP:journals/corr/AvrahamG17}).

In this article we consider a modification of
skip-gram that takes into account the hierarchical structure of the
text. Words co-occurrence is considered with respect to a
word position in a parse tree -- words form a parse-tree-gram 
(PT-gram) if they belong to the same sub-tree of a parse tree.
Instead of words in a fixed-size window we are first building
a parse tree of a sentence and analyze its structure.

To evaluate the effect of using PT-grams we've built
two word2vec embeddings -- using skip-grams and PT-grams representations --
and compared their performance on analogy test 
(\cite{DBLP:journals/corr/abs-1301-3781}).

\section{Calculations}

For the purposes of this study we were using relatively small
corpus of 1 million sentences from Wikipedia (\cite{leipzigcorpora}),
Stanford CoreNLP (\cite{corenlp}) to build parse trees, and
Tensorflow (\cite{tensorflow}) for word2vec program.

Parse trees split into subtrees form variable-size PT-grams
(see Appendix for an example).
Average size of subtrees is 4.2 words.
This size was used as a reference to construct
window of comparable size (-2,+2) for skip-grams. 
PT-grams for the example above:


Two embeddings (300-dimensional space)
were generated with the same parameters and models were trained
for the same epochs.

\section{Results}

\section{Conclusion}




\bibliographystyle{unsrt}
\bibliography{ptgrams1}

\newpage
\section{Appendix. PT-grams: example}

Below is the parse tree for the following sentence:
\textit{According to one of the legends in the Acts, Thomas was at first reluctant to accept this mission, but the Lord appeared to him in a night vision and said, "Fear not, Thomas".} 
\footnote{Wikipedia article about Thomas the Apostle}

\begin{scriptsize}
\tikzset{grow'=right} 
\tikzset{every tree node/.style={anchor=base west}} 
\Tree[.ROOT [.S [.S [.PP [.VBG According ] [.PP [.TO to ] [.NP [.NP [.CD one ] ] [.PP [.IN of ] [.NP [.NP [.DT the ] [.NNS legends ] ] [.PP [.IN in ] [.NP [.DT the ] [.NNS Acts ] ] ] ] ] ] ] ] [.NP [.NNP Thomas ] ] [.VP [.VBD was ] [.PP [.IN at ] [.NP [.NP [.JJ first ] ] [.ADJP [.JJ reluctant ] [.S [.VP [.TO to ] [.VP [.VB accept ] [.NP [.DT this ] [.NN mission ] ] ] ] ] ] ] ] ] ]  [.CC but ] [.S [.NP [.DT the ] [.NN Lord ] ] [.VP [.VP [.VBD appeared ] [.PP [.TO to ] [.NP [.PRP him ] ] ] [.PP [.IN in ] [.NP [.DT a ] [.NN night ] [.NN vision ] ] ] ] [.CC and ] [.VP [.VBD said ] [.S [.NP [.NP [.NN Fear ] [.RB not ] ] [.NP [.NNP Thomas ] ] ] ] ] ] ] ] ]

\end{scriptsize}


\end{document}
