\chapter{Multi-player games}

In this chapter we'll consider decision-making in a multi-palyer
games -- the case when the state of a game can change unpredictably
due to a move of a counterparty. Games like Chess, Checkers, Go
are all good examples of two-player games. Building a computer
program that can play these games is difficult, but ideas behind
several approaches can be studied on a game as simple as Tic-Tac-Toe.

\section{Tic-Tac-Toe -- the game}

Two players make moves on a 3x3 board. They start with an empty board

\begin{center}
\begin{tabular}{ l | c | r }
     &  &  \\ \hline
     &  &  \\ \hline
     &  &  \\
  \end{tabular}
\end{center}
First player puts X in a free space,
the second player puts O. The player who first puts 3 symbols in a row is
the winner. Examples of winning positions:

\begin{center}
\begin{tabular}{ l | c | r }
    X & O & O \\ \hline
     & X &  \\ \hline
     &  & X  \\
  \end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ l | c | r }
    X &  & X \\ \hline
    O & O & O \\ \hline
      & X & X  \\
  \end{tabular}
\end{center}

There may be conditions when the board is full and there is no winner (draw):

\begin{center}
\begin{tabular}{ l | c | r }
    X & O & X \\ \hline
    O & X & O \\ \hline
    O & X & X  \\
  \end{tabular}
\end{center}


We'll be describing the position on the board by the list of 9 symbols:
"X", "O", or "\ " (empty space). For example, three positions above will be
["X", "O", "O", "\ ", "X", "\ ", "\ ", "\ ", "X"],
["X", " ", "X", "O", "O", "O", "\ ", "X", "X"], and
["X", "O", "X", "O", "X", "O", "O", "X", "X"].

\begin{tcolorbox}
\textbf{Python:} to continue you need to learn elements of
object-oriented programming -- classes in Python, constructors, member
functions and variables.
\end{tcolorbox}

There are several approaches to building a computer program that can
play Tic-Tac-Toe or other similar two-player games:

\begin{enumerate}
\item We can hard-code some strategy -- a set of rules the program
applies to choose next step. For example, two obvious rules are:
"\textbf{win}: if you have two symbols in a row and
can put the third symbol, do and win",
"\textbf{don't let win}: if on the next step your competitor
can win by putting third symbol, prevent this".
It can be shown that Tic-Tac-Toe has a winning strategy.
\item The first approach is hard to generalize -- it's close to impossible
to apply it to a game
just a little more complex than Tic-Tac-Toe. We can introduce
a general way of choosing next move that increases the chances of
a win -- so called minimax approach.
\item We'll find that minimax may require a lot of resources to evaluate
a move and the next approach will be the reinforcement learning
(Q-learning) to construct an evaluation function. The program
will first play with itself for some time and will eventually build a
model that can play against a human or other program.
\end{enumerate}

Let's start with a simple rule-based strategy -- we'll
implement two rules we've mentioned above (win and don't let win)
and for all other cases the program will be doing a random move.
We'll leave the development of a stronger rule-based strategy for
the challenge in the end of the chapter.

We have to prepare several service elements:

\begin{enumerate}
\item The class that implements the board and board-related functions.
\item Two classes that implement a computer player and a human player.
Computer player is making a move based on one or the other algorithm, and
the human player enters moves from the console.
\item The class that implements the game -- the order of operations,
position evaluation, etc.
\end{enumerate}

All those entities -- board, players, game -- have own data and functionality
and it's convenient to organize it as class member variables and functions.

\subsection{The board}

Let's first review the functionality related to the board and
not related to player's decisions, to the order of players moves, etc.
Any implementation of the board has to be able to do the following:

\begin{enumerate}
\item Initialize the empty board.
\item Check if the board is filled -- no moves available, or there is a winner
\item Check if a player is a winner.
\item Change its own state on move.
\item Find all possible moves (indices of all empty cells).
\item Return human-readable representation.
\end{enumerate}

Here is the implementation:

\textbf{\_\_init\_\_} initializes buffer of 9 symbols of empty space.

\textbf{filled} checks number of empty spaces and winners.

\textbf{winner} checks if symbol \textbf{p} is the winer.

\textbf{on\_move} updates the buffer -- writes the symbol of the move
to the move position.

\textbf{moves} returns indices of empty spaces. Function \textbf{enumerate}
returns enumerated list, for example \textbf{enumerate(["a","b"])} returns
\textbf{[(0,"a"),(1,"b")]}.

And last function returns text representation of a board.

%%%
\newpage

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: board}]
class board:
    def __init__(self):
        self.empty = " "
        self.buffer = [self.empty] * 9

    def filled(self):
        return len([x for x in self.buffer  \
                    if x is self.empty])==0 \
               or self.winner("X") or self.winner("O")

    def winner(self,p):
        winning_lines = [[0,1,2],[3,4,5],[6,7,8],
                         [0,3,6],[1,4,7],[2,5,8],
                         [0,4,8],[2,4,6]]
        for line in winning_lines:
            if self.buffer[line[0]] == p and \
               self.buffer[line[1]] == p and \
               self.buffer[line[2]] == p:
                return True
        return False

    def on_move(self,move,symbol):
        self.buffer[move] = symbol

    def moves(self):
        return [c[0] for c in enumerate(self.buffer)\
                if c[1] is self.empty]

    def __repr__(self):
        return "{}|{}|{}\n-----\n{}|{}|{}\n-----\n{}|{}|{}".\
               format(*self.buffer)
\end{lstlisting}

\begin{tcolorbox}
\textbf{Assignment:} change the function \textbf{winner} -- do not use
hard-coded winning lines.
\end{tcolorbox}

\subsection{Players}

Next elements of the game are two players -- player-computer and
player-human. The former has to make moves based on some algorithm,
the latter is accepting moves from a human player. Let's assume
the computer always makes first move.

The base class:

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: player, base class}]
class player:
    def __init__(self,symbol):
        self.symbol = symbol

    def move(self,board):
        pass
\end{lstlisting}
We need only two functions -- initialization (assigning a symbol)
and making a move (doesn't do anything in the base class).

Two derived classes:

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: player-computer}]
class computer(player):
    def __init__(self):
        player.__init__(self,"X")

    def move(self,board):
        options = board.moves()
        return options[randint(0,len(options)-1)]
\end{lstlisting}

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: player accepting moves from a human}]
class human(player):
    def __init__(self):
        player.__init__(self,"O")
    
    def move(self,board):
        print board
        i = int(input(">"))
        if i not in board.moves(): 
            return self.board.move(board)
        return i
\end{lstlisting}

Initialization functions specify symbols.

Computer player makes move randomly -- first it takes all
available moves for the board and returns the value at a random index.

\textbf{move} function for a human player first prints the board and then
waits for the human input. If the input is not in the list of available
moves it calls itself to re-request the input from the player.

\subsection{The game}

And finally let's consider the implementation of the game.
We'll need several functions:
\begin{enumerate}
\item Initialization -- creating an empty board, and initializing players.
\item After each move we need to switch players.
\item Functions that control the game -- taking a move from a player
and updating the state of the game until the board is filled.
\item In the end of the game we want to show the final state.
\end{enumerate}

Here is the code:

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: the game}]
class TTTRandom:
    def __init__(self,player1,player2):
        self.player1 = player1
        self.player2 = player2
        self.board = board()

    def switch(self):
        # using a,b=b,a to swap two variables
        self.player1,self.player2=self.player2,self.player1

    def play(self):
        while not self.board.filled():
            move = self.player1.move(self.board)
            self.on_move(move)
\end{lstlisting}

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: the game - continued}]
    def on_move(self,move):
        self.board.on_move(move,self.player1.symbol)
        if self.board.filled():
            self.done()
        else:
            self.switch()

    def done(self):
        print "Done:"
        print self.board
\end{lstlisting}
Review the code and confirm that it implements all elements listed above.

And this is the code to play the game:

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: playing the game}]
p1 = computer()
p2 = human()
game = TTTRandom(p1,p2)
game.play()
\end{lstlisting}

\begin{tcolorbox}
\textbf{Assignments:}
\begin{enumerate}
\item Combine the code for the board, players, and the game. Play the game.
\item Change \textbf{move} function to support "win and don't let win"
strategy.
\end{enumerate}
\end{tcolorbox}


\section{Tic-Tac-Toe, minimax approach}

As we can see implementing rule-based strategy can be a problem
even for simple games like Tic-Tac-Toe. Small size of the board
simplifies development, but any changes in rules or in size
would require starting from scratch. At the same time the number
of possible moves is finite and we can try to review all possibilities
and to choose winning moves. The number of possibilities can be
huge for games like Chess, but for Tic-Tac-Toe we can consider
all possibilities and build an optimal player.

The problem with the development is that we don't know how an
opponent moves - the opponent may follow the strategy we don't know
in advance. To address the issue we must assume that both
players move optimally trying to maximize the reward.

Consider the following example.

\begin{figure}[H]
\centering
\Tree[ .Start [ .2  5 6 ] [ .4 1 10 ] ]
\end{figure}

This is the tree of the game. First player makes first move
(from \textbf{Start} to \textbf{2} or \textbf{4}), then the second
player makes a move. Numbers associated with graph nodes are rewards.

Two players play against each other. First player is trying to maximize
the final reward - the reward in the end of the game after the second
move. First player is a \textbf{maximizer}. Second player
is minimizer and is trying to minimize the final reward.

Let's see what can be a sequence of moves:

\begin{enumerate}
\item First player moves to \textbf{4}. In this case the second player will
choose \textbf{1} as it's the minimum.
\item First player moves to \textbf{2}. The second player will choose \textbf{5}.
\end{enumerate}

Assuming that the second player chooses optimally the first player should
move to \textbf{2} -- the final reward in this case is \textbf{5} compared
to \textbf{1} in the other case. The assumption of optimal player is
crucial here -- for example the second player choosing randomly may choose
\textbf{10}.

The idea of the approach can be expressed in the following way:
\textbf{a player must choose a move that minimizes the maximum
reward of an opponent}. Remember that we have players of two types --
minimizers and maximizers. A minimizer reads the rule as
\textbf{a move that maximizes the minimum reward of an opponent}.

For games like Tic-Tac-Toe the reward is known only in the end of the
game -- a player either wins or looses or the game ends with draw.
First player (X) is a maximizer and in the case it wins the reward
is +1. The second player is a minimizer and the reward is -1. Reward
at a draw is 0.

To apply this approach to Tic-Tac-Toe we need to construct a computer
player that analyzes the game tree (all possible moves) and each time
chooses the best \textbf{minimax} move. Each move creates a new position
on the board until the game is over. We can use recursion to
build the game tree and analyze results of a move.

The Python code is similar to the code we developed in the previous section,
but we need to make several changes:

\begin{enumerate}
\item class \textbf{board} needs a function that returns reward
depending on the state of the board.
\item class \textbf{computer} needs an implementation of \textbf{minimax}
function and changes in \textbf{move} function.
\end{enumerate}

Here is the code:

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: the board, reward function}]
class board:
    ...
    def reward(self):
        if self.winner("X"): return 1.0
        if self.winner("O"): return -1.0
        return 0.0
    ...
\end{lstlisting}
The function returns non-zero reward depending on the winner (X or O)
and zero in all other cases.

The function takes three parameters: \textbf{self} as 
any member function, \textbf{board} -- the same as in the previous
section, and \textbf{maximizing} -- boolean flag indicating a
maximizer or minimizer (\textbf{True} for maximizer, 
\textbf{False} for minimizer).

First we are getting the current reward
from the board. If the reward (\textbf{score}) is +1 or -1 (non-zero)
the function returns the value. 
If the score is 0 and the board is filled (draw) the function returns 0.
In all other cases we have moves to make. Each move returns new
score and we are choosing the best score. The notion of the best depends on
the \textbf{maximizing} flag -- it is \textbf{max} for a maximizer
and \textbf{min} for a minimizer.
Note that we are altering
\textbf{maximizing} flag -- if current player is a maximizer the next move
makes a minimizer and vice versa. When calling \textbf{self.minimax}
recursively we are using \textbf{not maximizing} expression to change
the type of player.

%%%
\newpage

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: minimax computer player}]
class computer(player):
    ...
    def minimax(self,board,maximizing):
        score = board.reward()
        if score != 0: return score
        if board.filled(): return 0
        if maximizing:
            best = -10000.0
            for m in board.moves():
                m_board = deepcopy(board)
                m_board.on_move(m,"X")
                best = max(best,\
                        self.minimax(m_board,not maximizing))
        else:
            best = 10000.0
            for m in board.moves():
                m_board = deepcopy(board)
                m_board.on_move(m,"O")
                best = min(best,\
                        self.minimax(m_board,not maximizing))
        return best
\end{lstlisting}

And the move function:
\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: computer player, move funtion}]
class computer(player):
    ...
    def move(self,board):
        moves = board.moves()
        mm = []
        for m in moves:
            m_board = deepcopy(board)
            m_board.on_move(m,self.symbol)
            mm.append( self.minimax(m_board,False) )
        mm_index = mm.index(max(mm))
        return moves[mm_index]
    ...
\end{lstlisting}
The list \textbf{mm} consists of minimax values of each possible
move. Note and the call of \textbf{self.minimax} is for minimizer --
the flag provided to the function is \textbf{False}. This is because
the computer player is a maximizer and the next move, is for minimizer.

The function creates a list of all minimax values and returns the move
that correspond to the maximum of minimax. If the same minimax value 
appears more than one time the function returns the first move 
associated with the value. 


\begin{tcolorbox}
\textbf{Assignments:}
Adjust the code from the previous section to incorporate changes
for the board and computer player. Play the game. You may find that
the first move takes time, that is expected and we'll address this
problem in the next section.
\end{tcolorbox}

The approach doesn't distinguish between cases when the player can
win in one or five moves. You may find that sometimes the computer
will not choose the quickest possible strategy. The reward is always 
the same -- +1 (or -1) -- and doesn't depend on the number of moves.
We can address this issue by penalizing moves. 

The recursive call
of \textbf{minimax} function can be changed by adding the depth of
recursion as a parameter. Consider as an example function that
returns factorial of a number:


\begin{lstlisting}[language=Python,style=codelst2,caption={Factorial calculations}]
def factorial(n):
    if n <= 1: return 1
    return n * factorial(n-1)
\end{lstlisting}
We can add depth in the following way:
\begin{lstlisting}[language=Python,style=codelst2,caption={Factorial calculations with depth printing}]
def factorial(n,depth):
    print depth
    if n <= 1: return 1
    return n * factorial(n-1,depth+1)
\end{lstlisting}
Try to call this function as \textbf{factorial(5,0)} -- it will
return $5!$ (120) and will print the depth of each 
function call starting from 0.

The depth is the number of function calls. In the case of \textbf{minimax}
function the depth is the number of moves -- each call corresponds to a
move. One way to take into account the number of steps is to reduce 
the reward by the number of steps. Now the reward of the maximizer is $+1$.
You can change the reward to $10$ and subtract the depth:

%%%
\newpage

\begin{lstlisting}[language=Python,style=codelst2,caption={Tic-Tac-Toe: taking into accout the number of steps}]
def minimax(self,board,maximizing,depth):
   ...
        best = max(best,self.minimax(m_board,not maximizing,depth+1))
   ...
   return best - depth
\end{lstlisting}
In this case the winning strategy that requires the least number of moves 
will be prioritized. 

\begin{tcolorbox}
\textbf{Assignment:} change the code of \textbf{minimax} function to
take into account the number of moves as discussed above.
\end{tcolorbox}


\section{Tic-Tac-Toe: Q-learning}

This section introduces an advanced topic of
Reinforcement Learning -- the technique in one or the other
form used in a wide range of applications from developing
an invincible chess program to teaching a robot to perform
back flip.

Tic-Tac-Toe is a game with small number of possible moves
and short horizon -- the game ends not later than in 9 moves.
This simplifies the development of learning algorithm.
The method we'll be using is called Q-learning and it's
rooted in Bellman's Principle of Optimality. If we have an
optimal sequence of decisions -- for example winning moves
in Tic-Tc-Toe -- then at each moment the remaining part of
the sequence is also optimal. If the sequence of moves
${m_1, m_2, \dots m_N}$ is an optimal one, then
the sequence ${m_i, m_{i+1}}, \dots m_N$ is also optimal
starting from any moment $i$. In other words, the optimal sequence doesn't
depend on previous steps.

Let's consider the sequence of actions (moves) ${a_i}$ and associated
sequence of states ${s_i}$. States are results of actions --
for Tic-Tac-Toe states are boards after a sequence of moves
and each new move changes the board:
$$
s_i \xrightarrow[a_i]{} s_{i+1}
$$
Each state $s_i$ is associated with a reward $r_i$. For Tic-Tac-Toe,
the reward is zero for all states except winning positions.

We want to find an approach (often called a policy $\pi$) that
maps all possible states $S$ to all actions $A$
$$
\pi: S \rightarrow A
$$
This policy gives us an action for a state. Our goal is to
create a policy that maximizes future rewards -- maximizes
our chances to win in a game.

Let's introduce future rewards $V$ as a sum of all rewards
associated with future states:
$$
V(s_i) = r_i + r_{i+1} + r_{i+1} + \dots
$$
More common is to discount future rewards:
$$
V(s_i) = r_i + \gamma \cdot r_{i+1} + \gamma^2 \cdot r_{i+2} + \dots
$$
where $\gamma$ is a number from 0 to 1, normally closer to 1.
$V$ depends on the policy $\pi$. We denote $V^{*}$ the
maximum possible future reward.

For the state $s$ action $a$ creates new state $s'$:
$$
s \xrightarrow[a]{} s'
$$
Let's introduce function $Q(s,a)$:
$$
Q(s,a) = r(s,a) + \gamma\cdot V^{*}(s')
$$
The function $Q$ is the sum of the current reward plus
discounted maximum future reward.
Taking into account that 
$$V^{*}(s') = \max_{a'}Q(s',a')$$
we can write:
$$
Q(a,a) = r(a,a) + \gamma\cdot\max_{a'}Q(s',a')
$$
This gives us recursive definition of $Q$.

We can use this definition to find an approximation for $Q$.
We'll start with some approximation $Q_1$ -- for example
$Q = 1$ for all states and actions.
If we are lucky and $Q_1$ is the exact solution
we'll have
$$
Q_1(a,a) = r(a,a) + \gamma\cdot\max_{a'}Q_1(s',a')
$$
But most probably the difference between right-hand side
and left-hand side of the equation is not zero:
$$
\Delta Q_1 = r(s,a) + \gamma\cdot\max_{a'}Q_1(s',a') - Q_1(s,a) 
$$
$$
\Delta Q_1 \ne 0
$$
Let's update our approximation of $Q$ in the following way:
$$
Q_2 = Q_1 + \alpha\cdot\Delta Q_1
$$
Parameter $\alpha$ is a small number that controls the rate of updates.
This gives us the next approximation and we'll find $\Delta Q_2$ and next
approximation of Q:
$$
\Delta Q_2 = r(s,a) + \gamma\cdot\max_{a'}Q_2(s',a') - Q_2(s,a)
$$
$$
Q_3 = Q_2 + \alpha\cdot\Delta Q_2
$$
Each step updates our approximation of $Q$. At the step $n$ we have we
have next approximation $Q_{n+1}$:
$$
Q_{n+1}(s,a) = Q_n(s,a) + \alpha\cdot ( r(s,a) + \gamma\cdot max_{a'} Q_n(s',a') - Q_n(s,a) )
$$
If we run this procedure long enough we may notice that values of
$\Delta Q$ become smaller and smaller -- the process converges to some
$Q$\footnote{The question of convergence of this iterative procedure
is not covered in this course.}.

Let's use this approach to create (to learn) $Q$-function
for a Tic-Tac-Toe player. The player then will be choosing the move
that maximizes $Q$ for any given state (board).

The question is how to create an iterative procedure described above.
We can for example play against the computer and it will eventually
learn the results of moves at various positions of the board. This
is possible, but may take a lot of time and human efforts -- the number of
possible moves is big even for a simple game like Tic-Tac-Toe.

The alternative approach is to let computer play against itself
to learn the $Q$-function and use resulting function to play against
a human. We'll need to implement two modes of operation in the game:
learning and playing. During the learning two computer players
share the same $Q$-function, choose the best moves, and update
$Q$-function according to the resulting rewards. We can let them
play hundreds of thousands times to learn $Q$-function. To let
the player explore new positions we'll be doing random moves as well.
When playing against a human the computer player will be using
learned $Q$-function to choose the best moves.

The scheme described above requires several changes in the code
of Tic-Tac-Toe game.


















\section{Challenge}

In this chapter we've learned how to develop three types of
Tic-Tac-Toe players using rule-based approach, minimax, and Q-learning.
The challenge now is to compare them between each other and their
implementations by groups of students.

\begin{enumerate}
\item Implement rule-based Tic-Tac-Toe player. You already have the version
for "win, don't let win" strategy. Expand it by adding new rules -- use
your experience to come up with winning strategy.
Make the code reusable, so that computer players developed by
groups of students can compete.
\item Implement the game where rule-based player plays against
 minimax player.
\item Implement the game where the minimax player plays against 
Q-function player.
\item Experiment with parameters used to create Q-function (number of
games, $\gamma, \alpha, \epsilon$). Implement the game where two players
with different $Q$-functions play against each other.
\end{enumerate}









